{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 1979     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.3        |\n",
      "|    ep_rew_mean          | 26.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1423        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009547608 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.0024      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.23        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35          |\n",
      "|    ep_rew_mean          | 35          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1277        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008719214 |\n",
      "|    clip_fraction        | 0.0544      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.0885      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 42.8        |\n",
      "|    ep_rew_mean          | 42.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1249        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009277785 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 50.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 54.5        |\n",
      "|    ep_rew_mean          | 54.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1223        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008536554 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 57.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 68.6       |\n",
      "|    ep_rew_mean          | 68.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1161       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01127755 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.583     |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.5       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    value_loss           | 56.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 85          |\n",
      "|    ep_rew_mean          | 85          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1144        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008802359 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1132        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010595853 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.93        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 119         |\n",
      "|    ep_rew_mean          | 119         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1129        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005232284 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.41        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00914    |\n",
      "|    value_loss           | 37.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 137         |\n",
      "|    ep_rew_mean          | 137         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1126        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010168821 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 153         |\n",
      "|    ep_rew_mean          | 153         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1128        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002038759 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 71.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 175         |\n",
      "|    ep_rew_mean          | 175         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1126        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003237165 |\n",
      "|    clip_fraction        | 0.031       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.653       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 8.2         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 194          |\n",
      "|    ep_rew_mean          | 194          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1104         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047047213 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.515       |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.435        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 4.94         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 212          |\n",
      "|    ep_rew_mean          | 212          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1092         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031654723 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.512       |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.22         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    value_loss           | 3.21         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 230          |\n",
      "|    ep_rew_mean          | 230          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1105         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043038605 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | 0.206        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.134        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 1.95         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ppo_expert = PPO('MlpPolicy', env_id, verbose=1)\n",
    "ppo_expert.learn(total_timesteps=3e4)\n",
    "ppo_expert.save(\"ppo_expert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\stable_baseline_test\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward = 500.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "a2c_student = A2C('MlpPolicy', env_id, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:14<00:00, 2827.61it/s]\n"
     ]
    }
   ],
   "source": [
    "num_interactions = int(4e4)\n",
    "\n",
    "if isinstance(env.action_space, gym.spaces.Box):\n",
    "   expert_observations = np.empty((num_interactions,)   +env.observation_space.shape)\n",
    "   expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],))\n",
    "else:\n",
    "   expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "   expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
    "\n",
    "obs = env.reset()\n",
    "# tqdm => Progress Bar setting\n",
    "\n",
    "# 학습한 PPO 전문가 예상값 (데이터) 저장\n",
    "for i in tqdm(range(num_interactions)):\n",
    "   action, _ = ppo_expert.predict(obs, deterministic=True)\n",
    "   expert_observations[i] = obs\n",
    "   expert_actions[i] = action\n",
    "   obs, reward, done, info = env.step(action)\n",
    "if done:\n",
    "      obs = env.reset()\n",
    "      \n",
    "# savez_compressed => 여러개의 배열을 1개의 압축된 npz포맷 파일로 저장\n",
    "np.savez_compressed(\n",
    "   \"expert_data\",\n",
    "   expert_actions=expert_actions,\n",
    "   expert_observations=expert_observations,\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "   def __init__(self, expert_observations, expert_actions):\n",
    "      self.observations = expert_observations\n",
    "      self.actions = expert_actions\n",
    "   def __getitem__(self, index):\n",
    "      return (self.observations[index], self.actions[index])\n",
    "   def __len__(self):\n",
    "      return len(self.observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split expert data to training and test datasets\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "test_size = len(expert_dataset) - train_size\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "     expert_dataset, [train_size, test_size]\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_expert_dataset:  8000\n",
      "train_expert_dataset:  32000\n"
     ]
    }
   ],
   "source": [
    "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "print(\"train_expert_dataset: \", len(train_expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,):\n",
    "    \n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "    \n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)               \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "            else:\n",
    "                dist = model.get_distribution(data)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target = target.long()\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                  # A2C/PPO policy outputs actions, values, log_prob\n",
    "                  # SAC/TD3 policy outputs actions only\n",
    "                  if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                  else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                  action_prediction = action.double()\n",
    "                else:\n",
    "                  # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                  dist = model.get_distribution(data)\n",
    "                  action_prediction = dist.distribution.logits\n",
    "                  target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    a2c_student.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/32000 (0%)]\tLoss: 0.693924\n",
      "Train Epoch: 1 [6400/32000 (20%)]\tLoss: 0.640212\n",
      "Train Epoch: 1 [12800/32000 (40%)]\tLoss: 0.347415\n",
      "Train Epoch: 1 [19200/32000 (60%)]\tLoss: 0.454745\n",
      "Train Epoch: 1 [25600/32000 (80%)]\tLoss: 0.644650\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 2 [0/32000 (0%)]\tLoss: 0.587214\n",
      "Train Epoch: 2 [6400/32000 (20%)]\tLoss: 0.177221\n",
      "Train Epoch: 2 [12800/32000 (40%)]\tLoss: 0.212071\n",
      "Train Epoch: 2 [19200/32000 (60%)]\tLoss: 0.169936\n",
      "Train Epoch: 2 [25600/32000 (80%)]\tLoss: 0.154499\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 3 [0/32000 (0%)]\tLoss: 0.266309\n",
      "Train Epoch: 3 [6400/32000 (20%)]\tLoss: 0.149105\n",
      "Train Epoch: 3 [12800/32000 (40%)]\tLoss: 0.176475\n",
      "Train Epoch: 3 [19200/32000 (60%)]\tLoss: 0.172273\n",
      "Train Epoch: 3 [25600/32000 (80%)]\tLoss: 0.085955\n",
      "Test set: Average loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "pretrain_agent(\n",
    "    a2c_student,\n",
    "    epochs=3,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=1000,\n",
    ")\n",
    "a2c_student.save(\"a2c_student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward = 254.8 +/- 89.06042892328782\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_baseline_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
